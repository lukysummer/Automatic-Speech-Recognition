{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice User Interfaces: Speech Recognition with Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2023 total training examples.\n"
     ]
    }
   ],
   "source": [
    "from data_generator import vis_train_features\n",
    "\n",
    "# extract label and audio features for a single training example\n",
    "vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path = vis_train_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, Dense, Input, Dropout, TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "rnn (GRU)                    (None, None, 29)          16617     \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 16,617\n",
      "Trainable params: 16,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def simple_rnn_model(input_dim, \n",
    "                     output_dim = 29):\n",
    "\n",
    "    input_data = Input(shape = (None, input_dim),\n",
    "                       name = 'input')\n",
    "    \n",
    "    all_hidden = GRU(output_dim, \n",
    "                     return_sequences = True, \n",
    "                     implementation = 2,       # for hardware application\n",
    "                     name = 'rnn')(input_data)\n",
    "    \n",
    "    y_pred = Activation('softmax', \n",
    "                        name ='softmax')(all_hidden)\n",
    "    \n",
    "    model = Model(inputs = input_data, \n",
    "                  outputs = y_pred)\n",
    "    \n",
    "    model.output_length = lambda x: x\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model_0 = simple_rnn_model(input_dim = 161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "101/101 [==============================] - 312s 3s/step - loss: 854.4880 - val_loss: 758.0665\n",
      "Epoch 2/20\n",
      "101/101 [==============================] - 291s 3s/step - loss: 779.9370 - val_loss: 763.1588\n",
      "Epoch 3/20\n",
      "101/101 [==============================] - 286s 3s/step - loss: 778.8877 - val_loss: 754.9002\n",
      "Epoch 4/20\n",
      "101/101 [==============================] - 289s 3s/step - loss: 779.2077 - val_loss: 763.4951\n",
      "Epoch 5/20\n",
      "101/101 [==============================] - 286s 3s/step - loss: 779.0775 - val_loss: 758.1530\n",
      "Epoch 6/20\n",
      "101/101 [==============================] - 286s 3s/step - loss: 779.3403 - val_loss: 751.5135\n",
      "Epoch 7/20\n",
      "101/101 [==============================] - 287s 3s/step - loss: 779.7538 - val_loss: 761.4883\n",
      "Epoch 8/20\n",
      "101/101 [==============================] - 286s 3s/step - loss: 779.4923 - val_loss: 752.8776\n",
      "Epoch 9/20\n",
      "101/101 [==============================] - 288s 3s/step - loss: 779.1452 - val_loss: 760.2652\n",
      "Epoch 10/20\n",
      "101/101 [==============================] - 291s 3s/step - loss: 779.3788 - val_loss: 758.2696\n",
      "Epoch 11/20\n",
      "101/101 [==============================] - 291s 3s/step - loss: 778.3826 - val_loss: 759.4576\n",
      "Epoch 12/20\n",
      "101/101 [==============================] - 290s 3s/step - loss: 779.3000 - val_loss: 757.2227\n",
      "Epoch 13/20\n",
      "101/101 [==============================] - 290s 3s/step - loss: 779.4715 - val_loss: 758.7979\n",
      "Epoch 14/20\n",
      "101/101 [==============================] - 288s 3s/step - loss: 778.9857 - val_loss: 752.1644\n",
      "Epoch 15/20\n",
      "101/101 [==============================] - 292s 3s/step - loss: 779.1911 - val_loss: 766.1160\n",
      "Epoch 16/20\n",
      "101/101 [==============================] - 288s 3s/step - loss: 779.2525 - val_loss: 750.2778\n",
      "Epoch 17/20\n",
      "101/101 [==============================] - 290s 3s/step - loss: 779.0636 - val_loss: 757.1765\n",
      "Epoch 18/20\n",
      "101/101 [==============================] - 287s 3s/step - loss: 778.5637 - val_loss: 755.0629\n",
      "Epoch 19/20\n",
      "101/101 [==============================] - 292s 3s/step - loss: 779.0658 - val_loss: 758.5125\n",
      "Epoch 20/20\n",
      "101/101 [==============================] - 288s 3s/step - loss: 779.4108 - val_loss: 760.2399\n"
     ]
    }
   ],
   "source": [
    "from train_utils import train_model\n",
    "\n",
    "train_model(input_to_softmax = model_0, \n",
    "            pickle_path = 'model_0.pickle', \n",
    "            save_model_path = 'model_0.h5',\n",
    "            spectrogram = True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN + TimeDistributed Dense\n",
    "\n",
    "<img src=\"images/rnn_model.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "<img src=\"images/rnn_model_unrolled.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "rnn (GRU)                    (None, None, 200)         217200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "dense_layer (TimeDistributed (None, None, 29)          5829      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 223,829\n",
      "Trainable params: 223,429\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def rnn_model(input_dim, \n",
    "              units, \n",
    "              activation, \n",
    "              output_dim = 29):\n",
    "    \n",
    "    input_data = Input(name = 'input', \n",
    "                       shape = (None, input_dim))\n",
    "    \n",
    "    all_hidden = GRU(units = units, \n",
    "                     activation = activation,\n",
    "                     return_sequences = True, \n",
    "                     implementation = 2, \n",
    "                     name = 'rnn')(input_data)\n",
    "    \n",
    "    bn_all_hidden = BatchNormalization()(all_hidden)\n",
    "\n",
    "    logits = TimeDistributed(Dense(output_dim),\n",
    "                             name = \"dense_layer\")(bn_all_hidden)\n",
    "    \n",
    "    y_pred = Activation('softmax', name='softmax')(logits)\n",
    "    \n",
    "    model = Model(inputs = input_data, \n",
    "                  outputs = y_pred)\n",
    "    \n",
    "    model.output_length = lambda x: x\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model_1 = rnn_model(input_dim = 161, # change to 13 if you would like to use MFCC features\n",
    "                    units = 200,\n",
    "                    activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/101 [============================>.] - ETA: 2s - loss: 293.2429Epoch 00001: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 261s 3s/step - loss: 292.3242 - val_loss: 303.7291\n",
      "Epoch 2/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 227.9441Epoch 00002: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 227s 2s/step - loss: 227.9502 - val_loss: 230.3270\n",
      "Epoch 3/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 213.1416Epoch 00003: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 226s 2s/step - loss: 213.1040 - val_loss: 203.6049\n",
      "Epoch 4/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 180.4267Epoch 00004: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 225s 2s/step - loss: 180.4215 - val_loss: 174.7418\n",
      "Epoch 5/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 166.2093Epoch 00005: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 226s 2s/step - loss: 166.4594 - val_loss: 160.5611\n",
      "Epoch 6/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 157.7326Epoch 00006: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 228s 2s/step - loss: 157.9399 - val_loss: 157.7354\n",
      "Epoch 7/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 151.6849Epoch 00007: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 226s 2s/step - loss: 151.7240 - val_loss: 154.2967\n",
      "Epoch 8/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 147.1236Epoch 00008: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 225s 2s/step - loss: 147.1231 - val_loss: 152.2263\n",
      "Epoch 9/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 143.9379Epoch 00009: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 229s 2s/step - loss: 143.7917 - val_loss: 149.0940\n",
      "Epoch 10/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 141.3751Epoch 00010: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 227s 2s/step - loss: 141.3040 - val_loss: 144.5093\n",
      "Epoch 11/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 138.5528Epoch 00011: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 224s 2s/step - loss: 138.5976 - val_loss: 144.5996\n",
      "Epoch 12/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 136.0280Epoch 00012: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 229s 2s/step - loss: 136.1737 - val_loss: 142.1446\n",
      "Epoch 13/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 134.7566Epoch 00013: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 225s 2s/step - loss: 134.6507 - val_loss: 145.6504\n",
      "Epoch 14/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 133.3875Epoch 00014: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 227s 2s/step - loss: 133.3764 - val_loss: 145.4543\n",
      "Epoch 15/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 132.2733Epoch 00015: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 224s 2s/step - loss: 132.2202 - val_loss: 142.7484\n",
      "Epoch 16/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 131.5303Epoch 00016: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 220s 2s/step - loss: 131.4370 - val_loss: 143.5570\n",
      "Epoch 17/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 132.2395Epoch 00017: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 219s 2s/step - loss: 132.2364 - val_loss: 145.3494\n",
      "Epoch 18/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 133.6536Epoch 00018: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 225s 2s/step - loss: 133.8785 - val_loss: 148.3595\n",
      "Epoch 19/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 131.8215Epoch 00019: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 225s 2s/step - loss: 131.7380 - val_loss: 142.6753\n",
      "Epoch 20/20\n",
      "100/101 [============================>.] - ETA: 1s - loss: 131.1988Epoch 00020: saving model to results/model_1.h5\n",
      "101/101 [==============================] - 229s 2s/step - loss: 131.3174 - val_loss: 139.6823\n"
     ]
    }
   ],
   "source": [
    "from train_utils import train_model\n",
    "\n",
    "train_model(input_to_softmax = model_1, \n",
    "            pickle_path = 'model_1.pickle', \n",
    "            save_model_path = 'model_1.h5',\n",
    "            spectrogram = True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: 1D CNN (over temporal dimension) + RNN + TimeDistributed Dense\n",
    "\n",
    "<img src=\"images/cnn_rnn_model.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 200)         354400    \n",
      "_________________________________________________________________\n",
      "bn_conv_1d (BatchNormalizati (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "rnn (GRU)                    (None, None, 200)         240600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 29)          5829      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 602,429\n",
      "Trainable params: 601,629\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def cnn_rnn_model(input_dim, \n",
    "                  filters, \n",
    "                  kernel_size, \n",
    "                  conv_stride,\n",
    "                  conv_border_mode, \n",
    "                  units, \n",
    "                  output_dim=29):\n",
    "    \n",
    "    input_data = Input(name='input', shape=(None, input_dim))\n",
    "    \n",
    "    conv_1d = Conv1D(filters, \n",
    "                     kernel_size, \n",
    "                     strides = conv_stride, \n",
    "                     padding = conv_border_mode,\n",
    "                     activation = 'relu',\n",
    "                     name = 'conv1d')(input_data)\n",
    "    \n",
    "    bn_cnn = BatchNormalization(name='bn_conv_1d')(conv_1d)\n",
    "    \n",
    "    simp_rnn = GRU(units, \n",
    "                   activation='relu',\n",
    "                   return_sequences=True, \n",
    "                   implementation=2, \n",
    "                   name='rnn')(bn_cnn)\n",
    "    \n",
    "    bn_rnn = BatchNormalization()(simp_rnn)\n",
    "    \n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    \n",
    "    y_pred = Activation('softmax', \n",
    "                        name='softmax')(time_dense)\n",
    "    \n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    \n",
    "    model.output_length = lambda x: cnn_output_length(x, \n",
    "                                                      kernel_size, \n",
    "                                                      conv_border_mode, \n",
    "                                                      conv_stride)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def cnn_output_length(input_length, \n",
    "                      filter_size, \n",
    "                      border_mode, \n",
    "                      stride,\n",
    "                      dilation = 1):\n",
    "    \"\"\" Computes the length of the output sequence after 1D convolution along time\n",
    "    Params:\n",
    "        input_length (int): Length of the input sequence.\n",
    "        filter_size (int): Width of the convolution kernel.\n",
    "        border_mode (str): Only support `same` or `valid`.\n",
    "        stride (int): Stride size used in 1D convolution.\n",
    "        dilation (int)\n",
    "    \"\"\"\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    \n",
    "    assert border_mode in {'same', 'valid'}\n",
    "    \n",
    "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "    \n",
    "    if border_mode == 'same':\n",
    "        output_length = input_length\n",
    "    \n",
    "    elif border_mode == 'valid':\n",
    "        output_length = input_length - dilated_filter_size + 1\n",
    "\n",
    "    return (output_length + stride - 1) // stride\n",
    "\n",
    "\n",
    "model_2 = cnn_rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                        filters=200,\n",
    "                        kernel_size=11, \n",
    "                        conv_stride=2,\n",
    "                        conv_border_mode='valid',\n",
    "                        units=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 244.5861Epoch 00001: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 56s 558ms/step - loss: 243.7958 - val_loss: 232.1907\n",
      "Epoch 2/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 180.6530Epoch 00002: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 56s 551ms/step - loss: 180.3919 - val_loss: 253.2956\n",
      "Epoch 3/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 157.0823Epoch 00003: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 56s 552ms/step - loss: 157.1206 - val_loss: 258.2962\n",
      "Epoch 4/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 145.0791Epoch 00004: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 55s 546ms/step - loss: 145.0963 - val_loss: 250.3479\n",
      "Epoch 5/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 136.4723Epoch 00005: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 55s 547ms/step - loss: 136.3501 - val_loss: 229.6831\n",
      "Epoch 6/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 130.1194Epoch 00006: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 55s 549ms/step - loss: 129.8419 - val_loss: 211.8138\n",
      "Epoch 7/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 124.5096Epoch 00007: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 56s 555ms/step - loss: 124.3353 - val_loss: 192.6391\n",
      "Epoch 8/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 119.8477Epoch 00008: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 57s 566ms/step - loss: 119.8605 - val_loss: 168.5814\n",
      "Epoch 9/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 116.1618Epoch 00009: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 56s 557ms/step - loss: 115.9427 - val_loss: 151.3493\n",
      "Epoch 10/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 112.2349Epoch 00010: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 56s 558ms/step - loss: 112.2079 - val_loss: 143.1892\n",
      "Epoch 11/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 108.8938Epoch 00011: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 56s 551ms/step - loss: 108.8415 - val_loss: 134.5647\n",
      "Epoch 12/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 105.6103Epoch 00012: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 57s 563ms/step - loss: 105.6672 - val_loss: 139.7279\n",
      "Epoch 13/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 102.4341Epoch 00013: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 56s 551ms/step - loss: 102.4322 - val_loss: 136.9679\n",
      "Epoch 14/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 99.5602Epoch 00014: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 55s 548ms/step - loss: 99.6386 - val_loss: 136.6855\n",
      "Epoch 15/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 96.9791Epoch 00015: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 55s 546ms/step - loss: 97.1365 - val_loss: 137.8859\n",
      "Epoch 16/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 94.6777Epoch 00016: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 55s 542ms/step - loss: 94.7887 - val_loss: 140.7229\n",
      "Epoch 17/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 91.9463Epoch 00017: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 55s 544ms/step - loss: 91.9759 - val_loss: 140.1028\n",
      "Epoch 18/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 89.5296Epoch 00018: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 56s 550ms/step - loss: 89.5198 - val_loss: 147.7049\n",
      "Epoch 19/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 88.6921Epoch 00019: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 55s 542ms/step - loss: 88.6804 - val_loss: 144.3253\n",
      "Epoch 20/20\n",
      "100/101 [============================>.] - ETA: 0s - loss: 86.2043Epoch 00020: saving model to results/model_2.h5\n",
      "101/101 [==============================] - 55s 549ms/step - loss: 86.1241 - val_loss: 146.9234\n"
     ]
    }
   ],
   "source": [
    "from train_utils import train_model\n",
    "\n",
    "train_model(input_to_softmax=model_2, \n",
    "            pickle_path='model_2.pickle', \n",
    "            save_model_path='model_2.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Deeper RNN + TimeDistributed Dense\n",
    "\n",
    "<img src=\"images/deep_rnn_model.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, None, 200)         459400    \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, None, 29)          5829      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 465,229\n",
      "Trainable params: 464,429\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def deep_rnn_model(input_dim, \n",
    "                   units, \n",
    "                   recur_layers, \n",
    "                   output_dim = 29):\n",
    "\n",
    "    input_data = Input(name='the_input', \n",
    "                       shape=(None, input_dim))\n",
    "    \n",
    "    rnn_model = Sequential()\n",
    "    \n",
    "    for i in range(recur_layers):\n",
    "        rnn_model.add(GRU(units,\n",
    "                          return_sequences = True,\n",
    "                          implementation = 2,\n",
    "                          name = \"rnn_\" + str(i+1),\n",
    "                          input_shape=(None, input_dim)))\n",
    "        \n",
    "        rnn_model.add(BatchNormalization())\n",
    "        \n",
    "    all_hidden = rnn_model(input_data)\n",
    "    \n",
    "    time_dense = TimeDistributed(Dense(output_dim))(all_hidden)\n",
    "    \n",
    "    y_pred = Activation('softmax', \n",
    "                        name='softmax')(time_dense)\n",
    "    \n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    \n",
    "    model.output_length = lambda x: x\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model_3 = deep_rnn_model(input_dim = 161, # change to 13 if you would like to use MFCC features\n",
    "                         units = 200,\n",
    "                         recur_layers = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 344.5216Epoch 00001: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 393s 4s/step - loss: 343.3273 - val_loss: 259.3454\n",
      "Epoch 2/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 249.3570Epoch 00002: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 404s 4s/step - loss: 249.4263 - val_loss: 223.0606\n",
      "Epoch 3/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 227.4499Epoch 00003: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 405s 4s/step - loss: 227.3845 - val_loss: 203.1664\n",
      "Epoch 4/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 202.0703Epoch 00004: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 402s 4s/step - loss: 202.0352 - val_loss: 186.5340\n",
      "Epoch 5/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 178.4929Epoch 00005: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 400s 4s/step - loss: 178.4382 - val_loss: 166.3059\n",
      "Epoch 6/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 163.7093Epoch 00006: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 401s 4s/step - loss: 163.8935 - val_loss: 157.4520\n",
      "Epoch 7/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 153.9810Epoch 00007: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 403s 4s/step - loss: 154.1755 - val_loss: 153.4128\n",
      "Epoch 8/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 146.5280Epoch 00008: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 408s 4s/step - loss: 146.4720 - val_loss: 149.8578\n",
      "Epoch 9/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 140.2091Epoch 00009: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 406s 4s/step - loss: 140.0361 - val_loss: 146.7813\n",
      "Epoch 10/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 134.2780Epoch 00010: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 408s 4s/step - loss: 134.4252 - val_loss: 144.1271\n",
      "Epoch 11/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 129.6542Epoch 00011: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 405s 4s/step - loss: 129.6929 - val_loss: 142.1547\n",
      "Epoch 12/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 125.3337Epoch 00012: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 408s 4s/step - loss: 125.2752 - val_loss: 137.3014\n",
      "Epoch 13/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 121.2436Epoch 00013: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 408s 4s/step - loss: 121.4223 - val_loss: 138.9191\n",
      "Epoch 14/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 117.4239Epoch 00014: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 403s 4s/step - loss: 117.5040 - val_loss: 136.3815\n",
      "Epoch 15/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 114.8013Epoch 00015: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 400s 4s/step - loss: 114.8069 - val_loss: 138.3259\n",
      "Epoch 16/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 110.9920Epoch 00016: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 405s 4s/step - loss: 111.0904 - val_loss: 134.2974\n",
      "Epoch 17/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 107.9492Epoch 00017: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 404s 4s/step - loss: 107.9385 - val_loss: 137.3662\n",
      "Epoch 18/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 104.5537Epoch 00018: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 404s 4s/step - loss: 104.6296 - val_loss: 138.2000\n",
      "Epoch 19/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 102.2611Epoch 00019: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 402s 4s/step - loss: 102.2900 - val_loss: 134.6892\n",
      "Epoch 20/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 99.3324Epoch 00020: saving model to results/model_3.h5\n",
      "101/101 [==============================] - 402s 4s/step - loss: 99.3480 - val_loss: 137.1746\n"
     ]
    }
   ],
   "source": [
    "from train_utils import train_model\n",
    "\n",
    "train_model(input_to_softmax=model_3, \n",
    "            pickle_path='model_3.pickle', \n",
    "            save_model_path='model_3.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Bidirectional RNN + TimeDistributed Dense\n",
    "\n",
    "<img src=\"images/bidirectional_rnn_model.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 400)         434400    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 29)          11629     \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 446,029\n",
      "Trainable params: 446,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def bidirectional_rnn_model(input_dim, \n",
    "                            units, \n",
    "                            output_dim = 29):\n",
    "    \n",
    "    input_data = Input(name = 'the_input', \n",
    "                       shape = (None, input_dim))\n",
    "    \n",
    "    bidir_rnn = Bidirectional(GRU(units,\n",
    "                                  return_sequences = True,\n",
    "                                  implementation = 2,\n",
    "                                  name = 'rnn'))(input_data)\n",
    "    \n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bidir_rnn)\n",
    "    \n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    \n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    \n",
    "    model.output_length = lambda x: x\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model_4 = bidirectional_rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                                  units=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 310.9759Epoch 00001: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 402s 4s/step - loss: 309.9350 - val_loss: 247.2053\n",
      "Epoch 2/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 243.9554Epoch 00002: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 414s 4s/step - loss: 243.4182 - val_loss: 223.5104\n",
      "Epoch 3/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 220.9416Epoch 00003: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 406s 4s/step - loss: 221.0344 - val_loss: 200.5468\n",
      "Epoch 4/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 201.8033Epoch 00004: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 405s 4s/step - loss: 201.7611 - val_loss: 191.3279\n",
      "Epoch 5/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 190.6505Epoch 00005: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 402s 4s/step - loss: 190.2740 - val_loss: 183.5528\n",
      "Epoch 6/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 180.8550Epoch 00006: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 398s 4s/step - loss: 181.1587 - val_loss: 176.3534\n",
      "Epoch 7/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 172.9427Epoch 00007: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 398s 4s/step - loss: 172.9373 - val_loss: 168.8543\n",
      "Epoch 8/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 165.2550Epoch 00008: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 414s 4s/step - loss: 165.3036 - val_loss: 164.2167\n",
      "Epoch 9/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 158.1458Epoch 00009: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 414s 4s/step - loss: 158.3670 - val_loss: 160.2063\n",
      "Epoch 10/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 151.9755Epoch 00010: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 402s 4s/step - loss: 152.0839 - val_loss: 156.5888\n",
      "Epoch 11/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 146.6744Epoch 00011: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 406s 4s/step - loss: 146.4308 - val_loss: 151.8179\n",
      "Epoch 12/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 141.4594Epoch 00012: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 401s 4s/step - loss: 141.3297 - val_loss: 150.2504\n",
      "Epoch 13/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 136.3242Epoch 00013: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 398s 4s/step - loss: 136.3863 - val_loss: 145.9737\n",
      "Epoch 14/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 132.1385Epoch 00014: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 403s 4s/step - loss: 132.1724 - val_loss: 145.8704\n",
      "Epoch 15/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 128.1840Epoch 00015: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 404s 4s/step - loss: 128.0848 - val_loss: 146.5287\n",
      "Epoch 16/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 123.9888Epoch 00016: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 402s 4s/step - loss: 124.2745 - val_loss: 143.3191\n",
      "Epoch 17/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 120.3217Epoch 00017: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 408s 4s/step - loss: 120.4073 - val_loss: 142.9278\n",
      "Epoch 18/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 117.2109Epoch 00018: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 408s 4s/step - loss: 116.9159 - val_loss: 142.6827\n",
      "Epoch 19/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 113.5495Epoch 00019: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 411s 4s/step - loss: 113.5946 - val_loss: 142.0735\n",
      "Epoch 20/20\n",
      "100/101 [============================>.] - ETA: 3s - loss: 110.3812Epoch 00020: saving model to results/model_4.h5\n",
      "101/101 [==============================] - 415s 4s/step - loss: 110.2085 - val_loss: 141.9821\n"
     ]
    }
   ],
   "source": [
    "from train_utils import train_model\n",
    "\n",
    "train_model(input_to_softmax=model_4, \n",
    "            pickle_path='model_4.pickle', \n",
    "            save_model_path='model_4.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments \n",
    "\n",
    "Model_2 **(CNN + BDRNN)** has the **Lowest TRAINING Loss**. This explains that 1D Temporal Convolution really helped to extract existing patterns in the audio.\n",
    "\n",
    "Model_3 **(Deep RNN + TimeDistributed Dense)** has the **Lowest VALIDATION Loss**. This exaplains that having a stack of RNNs with Batch Normalization in between greatly improves the learning of temporal patterns. Considering that Model_4 **(Bi-directional RNN + TimeDistributed Dense)** performed quite well too, it would be a good idea to stack multiple bi-directional RNNs together for the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs  ->  Bidirectional LSTMs  ->  FCs\n",
    "I will try 2 versions of the final model, and use the one with better results for prediction.\n",
    "## (version 1) for CNNs: Batch Normalization -> Dropout -> Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      "the_input (InputLayer)                           (None, None, 161)                           0                \n",
      "______________________________________________________________________________________________________________\n",
      "cnn (Conv1D)                                     (None, None, 200)                           354400           \n",
      "______________________________________________________________________________________________________________\n",
      "dropout_cnn (Dropout)                            (None, None, 200)                           0                \n",
      "______________________________________________________________________________________________________________\n",
      "bn_cnn (BatchNormalization)                      (None, None, 200)                           800              \n",
      "______________________________________________________________________________________________________________\n",
      "bdrnn_1 (Bidirectional)                          (None, None, 200)                           641600           \n",
      "______________________________________________________________________________________________________________\n",
      "bn_rnn_1 (BatchNormalization)                    (None, None, 200)                           800              \n",
      "______________________________________________________________________________________________________________\n",
      "bdrnn_2 (Bidirectional)                          (None, None, 200)                           641600           \n",
      "______________________________________________________________________________________________________________\n",
      "bn_rnn_2 (BatchNormalization)                    (None, None, 200)                           800              \n",
      "______________________________________________________________________________________________________________\n",
      "td_dense_1 (TimeDistributed)                     (None, None, 200)                           40200            \n",
      "______________________________________________________________________________________________________________\n",
      "dropout_fc_1 (Dropout)                           (None, None, 200)                           0                \n",
      "______________________________________________________________________________________________________________\n",
      "fc_relu_1 (Activation)                           (None, None, 200)                           0                \n",
      "______________________________________________________________________________________________________________\n",
      "td_dense_2 (TimeDistributed)                     (None, None, 29)                            5829             \n",
      "______________________________________________________________________________________________________________\n",
      "fc_softmax (Activation)                          (None, None, 29)                            0                \n",
      "==============================================================================================================\n",
      "Total params: 1,686,029\n",
      "Trainable params: 1,684,829\n",
      "Non-trainable params: 1,200\n",
      "______________________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from ASR_model import ASR_network\n",
    "\n",
    "# If you are running the notebook not as the author, you have to adjsut the code in ASR_model to changed\n",
    "# the order of CNNs as Batch Normalization -> Dropout -> Activation\n",
    "model_end = ASR_network(n_input_channels = 161,\n",
    "                        n_cnn_filters = 200,\n",
    "                        kernel_size = 11, \n",
    "                        stride = 2, \n",
    "                        padding_mode = 'valid',\n",
    "                        dilation = 1,\n",
    "                        cnn_dropout = 0.3,\n",
    "                        n_bdrnn_layers = 2,\n",
    "                        n_hidden_rnn = 200,\n",
    "                        input_dropout = 0.3,      # dropout values referenced from: \n",
    "                        recurrent_dropout = 0.1,  # https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/\n",
    "                        rnn_merge_mode = 'sum',\n",
    "                        fc_n_hiddens = [200],\n",
    "                        fc_dropout = 0.3,\n",
    "                        output_dim = 29)\n",
    "\n",
    "# CNN: Batch Normalization -> Dropout -> Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute the code cell below to train the neural network you specified in `input_to_softmax`.  After the model has finished training, the model is [saved](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) in the HDF5 file `model_final.h5`.  The loss history is [saved](https://wiki.python.org/moin/UsingPickle) in `model_final.pickle`.  You are welcome to tweak any of the optional parameters while calling the `train_model` function, but this is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "101/101 [==============================] - 379s 4s/step - loss: 262.5128 - val_loss: 228.0045\n",
      "Epoch 2/20\n",
      "101/101 [==============================] - 385s 4s/step - loss: 225.7656 - val_loss: 210.4944\n",
      "Epoch 3/20\n",
      "101/101 [==============================] - 382s 4s/step - loss: 207.1325 - val_loss: 185.3815\n",
      "Epoch 4/20\n",
      "101/101 [==============================] - 390s 4s/step - loss: 186.1634 - val_loss: 164.1884\n",
      "Epoch 5/20\n",
      "101/101 [==============================] - 384s 4s/step - loss: 172.8958 - val_loss: 152.0083\n",
      "Epoch 6/20\n",
      "101/101 [==============================] - 384s 4s/step - loss: 163.3796 - val_loss: 144.3911\n",
      "Epoch 7/20\n",
      "101/101 [==============================] - 386s 4s/step - loss: 156.7961 - val_loss: 139.2801\n",
      "Epoch 8/20\n",
      "101/101 [==============================] - 388s 4s/step - loss: 151.4250 - val_loss: 138.2241\n",
      "Epoch 9/20\n",
      "101/101 [==============================] - 387s 4s/step - loss: 146.5611 - val_loss: 130.2297\n",
      "Epoch 10/20\n",
      "101/101 [==============================] - 387s 4s/step - loss: 142.6954 - val_loss: 128.6345\n",
      "Epoch 11/20\n",
      "101/101 [==============================] - 388s 4s/step - loss: 139.2354 - val_loss: 125.6789\n",
      "Epoch 12/20\n",
      "101/101 [==============================] - 383s 4s/step - loss: 135.3945 - val_loss: 123.5739\n",
      "Epoch 13/20\n",
      "101/101 [==============================] - 385s 4s/step - loss: 132.4733 - val_loss: 122.7951\n",
      "Epoch 14/20\n",
      "101/101 [==============================] - 384s 4s/step - loss: 129.6384 - val_loss: 120.0256\n",
      "Epoch 15/20\n",
      "101/101 [==============================] - 387s 4s/step - loss: 127.5497 - val_loss: 117.7965\n",
      "Epoch 16/20\n",
      "101/101 [==============================] - 385s 4s/step - loss: 125.2289 - val_loss: 118.5744\n",
      "Epoch 17/20\n",
      "101/101 [==============================] - 385s 4s/step - loss: 122.8782 - val_loss: 114.7185\n",
      "Epoch 18/20\n",
      "101/101 [==============================] - 384s 4s/step - loss: 120.7624 - val_loss: 113.3052\n",
      "Epoch 19/20\n",
      "101/101 [==============================] - 383s 4s/step - loss: 119.0097 - val_loss: 111.2418\n",
      "Epoch 20/20\n",
      "101/101 [==============================] - 386s 4s/step - loss: 116.7832 - val_loss: 111.0221\n"
     ]
    }
   ],
   "source": [
    "from train_utils import train_model\n",
    "\n",
    "train_model(input_to_softmax = model_end, \n",
    "            n_epochs = 20,\n",
    "            pickle_path = 'model_final.pickle', \n",
    "            save_model_path = 'model_final.h5',\n",
    "            spectrogram = True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (version 2) for CNNs: Activation -> Dropout -> Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      "the_input (InputLayer)                           (None, None, 161)                           0                \n",
      "______________________________________________________________________________________________________________\n",
      "cnn (Conv1D)                                     (None, None, 200)                           354400           \n",
      "______________________________________________________________________________________________________________\n",
      "dropout_cnn (Dropout)                            (None, None, 200)                           0                \n",
      "______________________________________________________________________________________________________________\n",
      "bn_cnn (BatchNormalization)                      (None, None, 200)                           800              \n",
      "______________________________________________________________________________________________________________\n",
      "bdrnn_1 (Bidirectional)                          (None, None, 200)                           641600           \n",
      "______________________________________________________________________________________________________________\n",
      "bn_rnn_1 (BatchNormalization)                    (None, None, 200)                           800              \n",
      "______________________________________________________________________________________________________________\n",
      "bdrnn_2 (Bidirectional)                          (None, None, 200)                           641600           \n",
      "______________________________________________________________________________________________________________\n",
      "bn_rnn_2 (BatchNormalization)                    (None, None, 200)                           800              \n",
      "______________________________________________________________________________________________________________\n",
      "td_dense_1 (TimeDistributed)                     (None, None, 200)                           40200            \n",
      "______________________________________________________________________________________________________________\n",
      "dropout_fc_1 (Dropout)                           (None, None, 200)                           0                \n",
      "______________________________________________________________________________________________________________\n",
      "fc_relu_1 (Activation)                           (None, None, 200)                           0                \n",
      "______________________________________________________________________________________________________________\n",
      "td_dense_2 (TimeDistributed)                     (None, None, 29)                            5829             \n",
      "______________________________________________________________________________________________________________\n",
      "fc_softmax (Activation)                          (None, None, 29)                            0                \n",
      "==============================================================================================================\n",
      "Total params: 1,686,029\n",
      "Trainable params: 1,684,829\n",
      "Non-trainable params: 1,200\n",
      "______________________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sample_models import *\n",
    "# specify the model\n",
    "model_end = final_model(n_input_channels = 161,\n",
    "                        n_cnn_filters = 200,\n",
    "                        kernel_size = 11, \n",
    "                        stride = 2, \n",
    "                        padding_mode = 'valid',\n",
    "                        dilation = 1,\n",
    "                        cnn_dropout = 0.3,\n",
    "                        n_bdrnn_layers = 2,\n",
    "                        n_hidden_rnn = 200,\n",
    "                        input_dropout = 0.3,      # dropout values referenced from: \n",
    "                        recurrent_dropout = 0.1,  # https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/\n",
    "                        rnn_merge_mode = 'sum',\n",
    "                        fc_n_hiddens = [200],\n",
    "                        fc_dropout = 0.3,\n",
    "                        output_dim = 29)\n",
    "\n",
    "# CNN: Activation -> Dropout -> Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "101/101 [==============================] - 383s 4s/step - loss: 242.3445 - val_loss: 218.9088\n",
      "Epoch 2/20\n",
      "101/101 [==============================] - 385s 4s/step - loss: 207.1750 - val_loss: 188.5485\n",
      "Epoch 3/20\n",
      "101/101 [==============================] - 387s 4s/step - loss: 184.4212 - val_loss: 162.4012\n",
      "Epoch 4/20\n",
      "101/101 [==============================] - 389s 4s/step - loss: 171.3718 - val_loss: 151.7362\n",
      "Epoch 5/20\n",
      "101/101 [==============================] - 387s 4s/step - loss: 161.5415 - val_loss: 144.8627\n",
      "Epoch 6/20\n",
      "101/101 [==============================] - 389s 4s/step - loss: 154.4133 - val_loss: 141.1182\n",
      "Epoch 7/20\n",
      "101/101 [==============================] - 383s 4s/step - loss: 148.3604 - val_loss: 130.9480\n",
      "Epoch 8/20\n",
      "101/101 [==============================] - 387s 4s/step - loss: 143.3829 - val_loss: 132.9221\n",
      "Epoch 9/20\n",
      "101/101 [==============================] - 384s 4s/step - loss: 139.4906 - val_loss: 125.7465\n",
      "Epoch 10/20\n",
      "101/101 [==============================] - 385s 4s/step - loss: 135.8064 - val_loss: 124.2169\n",
      "Epoch 11/20\n",
      "101/101 [==============================] - 386s 4s/step - loss: 132.2194 - val_loss: 120.9705\n",
      "Epoch 12/20\n",
      "101/101 [==============================] - 386s 4s/step - loss: 129.0538 - val_loss: 118.3115\n",
      "Epoch 13/20\n",
      "101/101 [==============================] - 385s 4s/step - loss: 126.5329 - val_loss: 119.2383\n",
      "Epoch 14/20\n",
      "101/101 [==============================] - 387s 4s/step - loss: 123.9305 - val_loss: 115.6327\n",
      "Epoch 15/20\n",
      "101/101 [==============================] - 386s 4s/step - loss: 121.2478 - val_loss: 115.7102\n",
      "Epoch 16/20\n",
      "101/101 [==============================] - 385s 4s/step - loss: 119.2195 - val_loss: 111.1495\n",
      "Epoch 17/20\n",
      "101/101 [==============================] - 383s 4s/step - loss: 117.0709 - val_loss: 113.4664\n",
      "Epoch 18/20\n",
      "101/101 [==============================] - 384s 4s/step - loss: 115.6936 - val_loss: 109.9083\n",
      "Epoch 19/20\n",
      "101/101 [==============================] - 383s 4s/step - loss: 113.5571 - val_loss: 107.7928\n",
      "Epoch 20/20\n",
      "101/101 [==============================] - 381s 4s/step - loss: 111.6465 - val_loss: 108.4377\n"
     ]
    }
   ],
   "source": [
    "from train_utils import train_model\n",
    "\n",
    "train_model(input_to_softmax = model_end, \n",
    "            n_epochs = 20,\n",
    "            pickle_path = 'model_final_20_epochs.pickle', \n",
    "            save_model_path = 'model_final_20_epochs.h5',\n",
    "            spectrogram = True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems like version 2 performed slightly better. Thus, I will train it for 10 more epochs.**\n",
    "\n",
    "## Train for 10 more epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_end.load_weights('results/model_final_20_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "101/101 [==============================] - 337s 3s/step - loss: 112.3603 - val_loss: 112.6928\n",
      "Epoch 2/10\n",
      "101/101 [==============================] - 344s 3s/step - loss: 110.0954 - val_loss: 106.7947\n",
      "Epoch 3/10\n",
      "101/101 [==============================] - 343s 3s/step - loss: 107.2232 - val_loss: 107.2683\n",
      "Epoch 4/10\n",
      "101/101 [==============================] - 343s 3s/step - loss: 106.2565 - val_loss: 104.5403\n",
      "Epoch 5/10\n",
      "101/101 [==============================] - 344s 3s/step - loss: 105.0265 - val_loss: 105.5926\n",
      "Epoch 6/10\n",
      "101/101 [==============================] - 343s 3s/step - loss: 103.3525 - val_loss: 102.6976\n",
      "Epoch 7/10\n",
      "101/101 [==============================] - 343s 3s/step - loss: 102.0559 - val_loss: 103.2383\n",
      "Epoch 8/10\n",
      "101/101 [==============================] - 342s 3s/step - loss: 101.3664 - val_loss: 103.2168\n",
      "Epoch 9/10\n",
      "101/101 [==============================] - 343s 3s/step - loss: 100.0951 - val_loss: 102.8656\n",
      "Epoch 10/10\n",
      "101/101 [==============================] - 343s 3s/step - loss: 98.9001 - val_loss: 102.5828\n"
     ]
    }
   ],
   "source": [
    "from train_utils import train_model\n",
    "\n",
    "train_model(input_to_softmax = model_end, \n",
    "            n_epochs = 10,\n",
    "            pickle_path = 'model_final_30_epochs.pickle', \n",
    "            save_model_path = 'model_final_30_epochs.h5',\n",
    "            spectrogram = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try training a little more with a smaller learning rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "101/101 [==============================] - 373s 4s/step - loss: 97.1051 - val_loss: 101.6542\n",
      "Epoch 2/10\n",
      "101/101 [==============================] - 371s 4s/step - loss: 94.0384 - val_loss: 99.9138\n",
      "Epoch 3/10\n",
      "101/101 [==============================] - 373s 4s/step - loss: 92.6980 - val_loss: 98.6469\n",
      "Epoch 4/10\n",
      "101/101 [==============================] - 370s 4s/step - loss: 91.8441 - val_loss: 99.4655\n",
      "Epoch 5/10\n",
      "101/101 [==============================] - 371s 4s/step - loss: 91.0889 - val_loss: 99.5015\n",
      "Epoch 6/10\n",
      "101/101 [==============================] - 372s 4s/step - loss: 90.3894 - val_loss: 97.2002\n",
      "Epoch 7/10\n",
      "101/101 [==============================] - 377s 4s/step - loss: 89.6950 - val_loss: 99.4448\n",
      "Epoch 8/10\n",
      "101/101 [==============================] - 376s 4s/step - loss: 89.1524 - val_loss: 98.8418\n",
      "Epoch 9/10\n",
      "101/101 [==============================] - 376s 4s/step - loss: 88.4495 - val_loss: 96.7925\n",
      "Epoch 10/10\n",
      "101/101 [==============================] - 375s 4s/step - loss: 87.7038 - val_loss: 98.3304\n"
     ]
    }
   ],
   "source": [
    "model_end.load_weights('results/model_final_30_epochs.h5')\n",
    "\n",
    "train_model(input_to_softmax = model_end, \n",
    "            n_epochs = 10,\n",
    "            pickle_path = 'model_final_40_epochs.pickle', \n",
    "            save_model_path = 'model_final_40_epochs.h5',\n",
    "            spectrogram = True,\n",
    "            lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try training a little more with even smaller learning rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "101/101 [==============================] - 362s 4s/step - loss: 87.1788 - val_loss: 97.0915\n",
      "Epoch 2/3\n",
      "101/101 [==============================] - 365s 4s/step - loss: 84.7045 - val_loss: 97.1530\n",
      "Epoch 3/3\n",
      "101/101 [==============================] - 366s 4s/step - loss: 84.1458 - val_loss: 96.1343\n"
     ]
    }
   ],
   "source": [
    "from train_utils import train_model\n",
    "model_end.load_weights('results/model_final_40_epochs.h5')\n",
    "\n",
    "train_model(input_to_softmax = model_end, \n",
    "            n_epochs = 3,\n",
    "            pickle_path = 'model_final_43_epochs.pickle', \n",
    "            save_model_path = 'model_final_43_epochs.h5',\n",
    "            spectrogram = True,\n",
    "            lr = 0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe this is enough since we don't want to overfit, and I am observing that the training loss continues to decrease steadily while the validation loss is going up and down even with decreased learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "My final model has the following architecture:\n",
    "\n",
    "1. **Input**: Spectrograms \n",
    "\n",
    "\n",
    "2. **One 1-D CNN Layer + Batch Normalization**:\n",
    "    * number of filters : 200\n",
    "    * activation : ReLU\n",
    "    * kernel_size : 11\n",
    "    * stride : 2\n",
    "    * dilation : 1\n",
    "    * dropout : 30%\n",
    "    * order: Activation -> Dropout -> Batch Normalization\n",
    "    \n",
    "    \n",
    "3. **Two Bi-directional LSTM Layers + Batch Normalizations**:\n",
    "    * number of hidden nodes : 200\n",
    "    * activation : Tanh\n",
    "    * input dropout : 30%\n",
    "    * recurrent cells dropout: 10%\n",
    "    * order: Activation -> Dropout -> Batch Normalization\n",
    "    \n",
    "    \n",
    "4. **First FC Layer**:\n",
    "    * number of hidden nodes: 200\n",
    "    * activation : ReLU\n",
    "    * dropout: 30%\n",
    "    \n",
    "    \n",
    "5. **Second FC Layer (for Output)**:\n",
    "    * number of hidden nodes: 29\n",
    "    * activation : Softmax\n",
    "\n",
    "\n",
    "Notes:\n",
    "* I used Spectrograms instead of MFCC as inputs to give 1-D CNN layer more features to work with.\n",
    "\n",
    "* I used Dropout for both CNN and RNN layers in order to prevent overfitting.\n",
    "\n",
    "* I only used one CNN layer and two BD-RNN layers considering the relatively small dataset size. \n",
    "\n",
    "Several things I learnt while building the model:\n",
    "* For RNNs' Activation Function, **Tanh** must be used and NOT ReLU. When I used ReLU, the training & validation loss was much higher for Epoch 1 and 2, and barely decreased after Epoch 2.\n",
    "\n",
    "* After CNN and RNN layers, **Batch Normalization** MUST be used in order to prevent exploding gradient and the losses being displayed as \"nan\".\n",
    "\n",
    "* Although my model dimensions were quite humble (only one CNN layer & two BD RNN layers, all with 200 hidden nodes), the whole training process of 30 epochs almost took 4 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## STEP 3: Obtain Predictions\n",
    "\n",
    "We have written a function for you to decode the predictions of your acoustic model.  To use the function, please execute the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_generator import AudioGenerator\n",
    "from keras import backend as K\n",
    "from utils import int_sequence_to_text\n",
    "from IPython.display import Audio\n",
    "\n",
    "def get_predictions(index, partition, input_to_softmax = model_end, model_path = 'results/model_final_43_epochs.h5'):\n",
    "    \"\"\" Print a model's decoded predictions\n",
    "    Params:\n",
    "        index (int): The example you would like to visualize\n",
    "        partition (str): One of 'train' or 'validation'\n",
    "        input_to_softmax (Model): The acoustic model\n",
    "        model_path (str): Path to saved acoustic model's weights\n",
    "    \"\"\"\n",
    "    # load the train and test data\n",
    "    data_gen = AudioGenerator()\n",
    "    data_gen.load_train_data()\n",
    "    data_gen.load_validation_data()\n",
    "    \n",
    "    # obtain the true transcription and the audio features \n",
    "    if partition == 'validation':\n",
    "        transcr = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    elif partition == 'train':\n",
    "        transcr = data_gen.train_texts[index]\n",
    "        audio_path = data_gen.train_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"train\" or \"validation\"')\n",
    "        \n",
    "    # obtain and decode the acoustic model's predictions\n",
    "    input_to_softmax.load_weights(model_path)\n",
    "    prediction = input_to_softmax.predict(np.expand_dims(data_point, axis=0))\n",
    "    output_length = [input_to_softmax.output_length(data_point.shape[0])] \n",
    "    pred_ints = (K.eval(K.ctc_decode(\n",
    "                prediction, output_length)[0][0])+1).flatten().tolist()\n",
    "    \n",
    "    # play the audio file, and display the true and predicted transcriptions\n",
    "    print('-'*80)\n",
    "    Audio(audio_path)\n",
    "    print('True transcription:\\n' + '\\n' + transcr)\n",
    "    print('-'*80)\n",
    "    print('Predicted transcription:\\n' + '\\n' + ''.join(int_sequence_to_text(pred_ints)))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "her father is a most remarkable person to say the least\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription:\n",
      "\n",
      "her fother s a mos ere markcabl person to sa the last\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "he gave thanks for our food and comfort and prayed for the poor and destitute in great cities where the struggle for life was harder than it was here with us\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription:\n",
      "\n",
      "he gave thaingk s for ar foodant comfeirt and prade fon the por indis tetu in gra sites whe the strole for lie was harder then it was here withtust\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "just smell the wild roses they are always so spicy after a rain\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription:\n",
      "\n",
      "jus mel a wil roses theya al by es o spi sy ateu ary\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_predictions(index = 0, partition = 'train')\n",
    "get_predictions(index = 1000, partition = 'train')\n",
    "get_predictions(index = 999, partition = 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One standard way to improve the results of the decoder is to incorporate a language model.  We won't pursue this in the notebook, but you are welcome to do so as an _optional extension_. \n",
    "\n",
    "If you are interested in creating models that provide improved transcriptions, you are encouraged to download [more data](http://www.openslr.org/12/) and train bigger, deeper models.  But beware - the model will likely take a long while to train.  For instance, training this [state-of-the-art](https://arxiv.org/pdf/1512.02595v1.pdf) model would take 3-6 weeks on a single GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancement 1: Spell Correction\n",
    "## (1.1) Spell Correction using the Training Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words:  [('the', 1851), ('and', 932), ('of', 828), ('to', 803), ('a', 687)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from data_generator import AudioGenerator\n",
    "\n",
    "corpus = AudioGenerator()\n",
    "corpus.load_train_data(\"train_corpus.json\")\n",
    "corpus_texts = corpus.train_texts\n",
    "corpus_tokens = np.concatenate(np.array([sentence.lower().split() for sentence in corpus_texts]))\n",
    "token_counter = Counter(corpus_tokens)\n",
    "print(\"most common words: \", token_counter.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell checker by Peter Norvig (http://norvig.com/spell-correct.html)\n",
    "# + my addition of vowel_replaces & conso_replaces for edits_1 function\n",
    "def word_probability(word):\n",
    "    return token_counter[word]/sum(token_counter.values())\n",
    "\n",
    "\n",
    "def edits_1(word):\n",
    "    ''' Performs ONE of deletion, transposition, replacement, or insertion to the given word '''\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    vowels = \"aeiouy\"\n",
    "    consonants = \"bcdfghjklmnpqrstvwxz\"\n",
    "    splits = [(word[1:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    deletes    = [word[0] + L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [word[0] + L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    vowel_replaces = [word[0] + L + v + R[1:]       for L, R in splits if (R and R[0] in vowels) for v in vowels]        \n",
    "    conso_replaces = [word[0] + L + c + R[1:]       for L, R in splits if (R and R[0] in consonants) for c in consonants]                                                              \n",
    "    inserts    = [word[0] + L + ch + R              for L, R in splits for ch in letters]\n",
    "\n",
    "    return set(deletes + transposes + vowel_replaces + conso_replaces + inserts)\n",
    "\n",
    "\n",
    "def edits_2(word):\n",
    "    ''' Performs TWO of deletion, transposition, replacement, or insertion to the given word '''\n",
    "    edits_1_words = edits_1(word)\n",
    "    edits_2 = set()\n",
    "    for e1 in edits_1_words:\n",
    "        for e2 in edits_1(e1):\n",
    "            edits_2.add(e2)\n",
    "    \n",
    "    return edits_2\n",
    "\n",
    "\n",
    "def existing_words(editted_words):\n",
    "    ''' Returns a subset of editted words that exist in train corpus '''\n",
    "    return set(w for w in editted_words if w in token_counter)\n",
    "\n",
    "\n",
    "def candidates(word):\n",
    "    ''' Returns possible spelling corrections for the given word: \n",
    "        A or B or C: \n",
    "            = A, if A is not an empty set \n",
    "            = B, if A is an empty set\n",
    "            = C, if A and B are empty sets\n",
    "        Thus, returns:\n",
    "            original word, if it exists in corpus (thus has correct spelling), otherwise\n",
    "            words editted once, if they exists in corpus, otherwise\n",
    "            word editted twice, if they exists in corpus, otherwise\n",
    "            original word, although it does not exist in corpus\n",
    "    '''\n",
    "    \n",
    "    return  existing_words([word]) or \\\n",
    "            existing_words(edits_2(word)) or \\\n",
    "            existing_words(edits_1(word)) or \\\n",
    "            [word]                               \n",
    "\n",
    "\n",
    "def correction(word):\n",
    "    ''' Returns the most probabale spelling correction of the given word'''\n",
    "\n",
    "    return max(candidates(word), key = word_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Adjust get_predictions function to include the spell correction:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_generator import AudioGenerator\n",
    "from keras import backend as K\n",
    "from utils import int_sequence_to_text\n",
    "from IPython.display import Audio\n",
    "\n",
    "def get_predictions(index, partition, ASR_model = model_end, model_path = 'results/model_final_43_epochs.h5'):\n",
    "    \"\"\" Print a model's decoded predictions\n",
    "    Params:\n",
    "        index (int): sample index of training or validation set\n",
    "        partition (str): One of 'train' or 'validation'\n",
    "        ASR_model (Model): The acoustic model\n",
    "        model_path (str): Path to saved acoustic model's weights\n",
    "    \"\"\"\n",
    "    # 1. Load the train and validation data\n",
    "    data_gen = AudioGenerator()\n",
    "    data_gen.load_train_data()\n",
    "    data_gen.load_validation_data()\n",
    "    \n",
    "    # 2. Obtain the true transcription and the audio features \n",
    "    if partition == 'train':\n",
    "        true_label = data_gen.train_texts[index]\n",
    "        audio_path = data_gen.train_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "        \n",
    "    elif partition == 'validation':\n",
    "        true_label = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))       \n",
    "    \n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"train\" or \"validation\"')\n",
    "        \n",
    "    # 3. Obtain and decode the acoustic model's predictions\n",
    "    ASR_model.load_weights(model_path)\n",
    "    prediction = ASR_model.predict(np.expand_dims(data_point, axis=0))  # give a batch size of 1\n",
    "    output_length = [ASR_model.output_length(data_point.shape[0])] \n",
    "    pred_ints = (K.eval(K.ctc_decode(prediction, output_length)[0][0]) + 1).flatten().tolist()\n",
    "    pred_words = ''.join(int_sequence_to_text(pred_ints)).split()\n",
    "    \n",
    "    # 4. Perform Spelling Correction\n",
    "    corrected_str = [correction(word) for word in pred_words]\n",
    "    \n",
    "    # 5. play the audio file, and display the true and predicted transcriptions\n",
    "    print('-'*80)\n",
    "    Audio(audio_path)\n",
    "    print('True transcription:\\n' + '\\n' + true_label)\n",
    "    print('-'*80)\n",
    "    print('Predicted transcription: (Original -- Spell Correction)\\n')\n",
    "    print(' '.join(pred_words))\n",
    "    print(' '.join(corrected_str))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with spell correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "her father is a most remarkable person to say the least\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription: (Original -- Spell Correction)\n",
      "\n",
      "her fother s a mos ere markcabl person to sa the last\n",
      "her father s a my ere markcabl person to she the last\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "he gave thanks for our food and comfort and prayed for the poor and destitute in great cities where the struggle for life was harder than it was here with us\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription: (Original -- Spell Correction)\n",
      "\n",
      "he gave thaingk s for ar foodant comfeirt and prade fon the por indis tetu in gra sites whe the strole for lie was harder then it was here withtust\n",
      "he gave think s for and foodant comfort and place for the put indian ten in go side were the store for lie was harder then it was here withtust\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "just smell the wild roses they are always so spicy after a rain\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription: (Original -- Spell Correction)\n",
      "\n",
      "jus mel a wil roses theya al by es o spi sy ateu ary\n",
      "just my a was roses the and by eyes o she she at a\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_predictions(index = 0, partition = 'train')\n",
    "get_predictions(index = 1000, partition = 'train')\n",
    "get_predictions(index = 999, partition = 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.2) Spell Correction using Brown Corpus\n",
    "\n",
    "Brown Corpus could be used instead if we have a smaller training data, since new speech could contain word that do not exist in training set vocabulary. However, if the training data was from a niche corpus where the vocabulary must be limited (e.g. medical document), using a general language corpus such as Brown Corpus for spelling correction might lead to incorrect results. \n",
    "\n",
    "When I experimented with Spelling Correction using either Training Corpus and Brown Corpus along with the 2 other enhancements below, it seemed that **using Training Corpus led to more accurate results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "brown_corpus = brown.words(categories = 'adventure') + brown.words(categories='romance') + brown.words(categories='fiction')\n",
    "token_counter = Counter(brown_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "her father is a most remarkable person to say the least\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription: (Original -- Spell Correction)\n",
      "\n",
      "her fother s a mos ere markcabl person to sa the last\n",
      "her father she a me even markcabl person to she the last\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "he gave thanks for our food and comfort and prayed for the poor and destitute in great cities where the struggle for life was harder than it was here with us\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription: (Original -- Spell Correction)\n",
      "\n",
      "he gave thaingk s for ar foodant comfeirt and prade fon the por indis tetu in gra sites whe the strole for lie was harder then it was here withtust\n",
      "he gave think she for and foodant comfort and place for the put insist ten in go side were the stone for lie was harder then it was here withtust\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "just smell the wild roses they are always so spicy after a rain\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription: (Original -- Spell Correction)\n",
      "\n",
      "jus mel a wil roses theya al by es o spi sy ateu ary\n",
      "just me a was roses the and by eyes of she she at a\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_predictions(index = 0, partition = 'train')\n",
    "get_predictions(index = 1000, partition = 'train')\n",
    "get_predictions(index = 999, partition = 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "The result still looks very bad. The corrected sentences do not make sense at all both globally and locally. This could be improved by looking at neighbouring words' part of speech, which gives a phrase a logical \"sense\" for human ears.\n",
    "\n",
    "# Enhancement 2: POS tagging \n",
    "### Suggest a better word choice for spell correction, looking at the **PREVIOUS WORD's Part of Speech**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram_tags model : \n",
    "### Bigram for POS tags (i.e. gives a probability of the current word's tag, given the previous word's tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 1. Create a bigram model for POS tags, base on Training Corpus\n",
    "bigram_tags = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# (1.1) Record bigram counts in a dict\n",
    "for sentence in corpus_texts:\n",
    "    tags = [nltk.pos_tag((w.split()))[0][1] for w in sentence.split()]\n",
    "    for t1, t2 in bigrams(tags):\n",
    "        bigram_tags[t1][t2] += 1\n",
    "        \n",
    "# (1.2) Transform the counts into probabilities\n",
    "for t1 in bigram_tags:\n",
    "    total_count = float(sum(bigram_tags[t1].values()))\n",
    "    for t2 in bigram_tags[t1]:\n",
    "        bigram_tags[t1][t2] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust candidates & correction function to include POS tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_POS(word, prev_tag=None):\n",
    "    if existing_words([word]):  # if predicted word appears in the corpus\n",
    "        if prev_tag is None:\n",
    "            return [word]\n",
    "        else:\n",
    "            tag = nltk.pos_tag((word.split()))[0][1]\n",
    "            if bigram_tags[prev_tag][tag] > 0.01:  # if the word has a reasonable tag, considering previous 2 words' tags\n",
    "                return [word]\n",
    "            \n",
    "    one_edits = existing_words(edits_1(word))\n",
    "    if one_edits:\n",
    "        if prev_tag is None:\n",
    "            return one_edits\n",
    "        else:\n",
    "            tags = [(w, nltk.pos_tag((w.split()))[0][1]) for w in one_edits]\n",
    "            bigrams = [(w, bigram_tags[prev_tag][tag]) for (w, tag) in tags]\n",
    "            logical_candidates = [pair[0] for pair in bigrams if pair[1] > 0.01]\n",
    "            if logical_candidates:\n",
    "                return logical_candidates\n",
    "\n",
    "    two_edits = existing_words(edits_2(word))\n",
    "    if two_edits:\n",
    "        if prev_tag is None:\n",
    "            return two_edits\n",
    "        else:\n",
    "            tags = [(w, nltk.pos_tag((w.split()))[0][1]) for w in two_edits]\n",
    "            bigrams = [(w, bigram_tags[prev_tag][tag]) for (w, tag) in tags]\n",
    "            logical_candidates = [pair[0] for pair in bigrams if pair[1] > 0.01]\n",
    "            if logical_candidates:\n",
    "                return logical_candidates\n",
    "    \n",
    "    return [word]   \n",
    "\n",
    "        \n",
    "def correction_POS(words):\n",
    "    ''' Returns the most probabale spelling correction of the given sentence (words)'''\n",
    "    corrected_sentence = []    \n",
    "    for word_i, word in enumerate(words): \n",
    "        # Current word's POS tag:\n",
    "        tag = nltk.pos_tag((word.split()))[0][1]\n",
    "        # Don't apply POS tagging selection to the first word\n",
    "        if word_i == 0:\n",
    "            prev_tag = None    \n",
    "\n",
    "        word_candidates = candidates_POS(word, prev_tag=prev_tag)\n",
    "        next_word = max(word_candidates, key = word_probability)\n",
    "        corrected_sentence.append(next_word) \n",
    "        prev_tag = nltk.pos_tag((next_word.split()))[0][1]\n",
    "            \n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust get_predictions function to include POS tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_generator import AudioGenerator\n",
    "from keras import backend as K\n",
    "from utils import int_sequence_to_text\n",
    "from IPython.display import Audio\n",
    "\n",
    "def get_predictions(index, partition, ASR_model = model_end, model_path = 'results/model_final_43_epochs.h5'):\n",
    "    \"\"\" Print a model's decoded predictions\n",
    "    Params:\n",
    "        index (int): sample index of training or validation set\n",
    "        partition (str): One of 'train' or 'validation'\n",
    "        ASR_model (Model): The acoustic model\n",
    "        model_path (str): Path to saved acoustic model's weights\n",
    "    \"\"\"\n",
    "    # 1. Load the train and validation data\n",
    "    data_gen = AudioGenerator()\n",
    "    data_gen.load_train_data()\n",
    "    data_gen.load_validation_data()\n",
    "    \n",
    "    # 2. Obtain the true transcription and the audio features \n",
    "    if partition == 'train':\n",
    "        true_label = data_gen.train_texts[index]\n",
    "        audio_path = data_gen.train_audio_paths[index]              \n",
    "        \n",
    "    elif partition == 'validation':\n",
    "        true_label = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]      \n",
    "    \n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"train\" or \"validation\"')\n",
    "    \n",
    "    data_point = data_gen.normalize(data_gen.featurize(audio_path)) \n",
    "    \n",
    "    # 3. Obtain and decode the acoustic model's predictions\n",
    "    ASR_model.load_weights(model_path)\n",
    "    prediction = ASR_model.predict(np.expand_dims(data_point, axis=0))  # give a batch size of 1\n",
    "    output_length = [ASR_model.output_length(data_point.shape[0])] \n",
    "    pred_ints = (K.eval(K.ctc_decode(prediction, output_length)[0][0]) + 1).flatten().tolist()\n",
    "    pred_words = ''.join(int_sequence_to_text(pred_ints)).split()\n",
    "    \n",
    "    # 4. Perform Spelling Correction (just for reference)\n",
    "    corrected_str = [correction(word) for word in pred_words]\n",
    "    \n",
    "    # 5. Perform Spelling Correction & POS Tagging\n",
    "    corrected_str_POS = correction_POS(pred_words)\n",
    "    \n",
    "    # 6. play the audio file, and display the true and predicted transcriptions\n",
    "    print('-'*80)\n",
    "    Audio(audio_path)\n",
    "    print('True transcription:\\n' + '\\n' + true_label)\n",
    "    print('-'*80)\n",
    "    print('Predicted transcription: (Original -- Spell Correction -- Spell Correction, POS tagging)\\n')\n",
    "    print(' '.join(pred_words))\n",
    "    print(' '.join(corrected_str))\n",
    "    print(' '.join(corrected_str_POS))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "her father is a most remarkable person to say the least\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription: (Original -- Spell Correction -- Spell Correction, POS tagging)\n",
      "\n",
      "her fother s a mos ere markcabl person to sa the last\n",
      "her father s a my ere markcabl person to she the last\n",
      "her father s a most ere markcabl person to say the last\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "he gave thanks for our food and comfort and prayed for the poor and destitute in great cities where the struggle for life was harder than it was here with us\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription: (Original -- Spell Correction -- Spell Correction, POS tagging)\n",
      "\n",
      "he gave thaingk s for ar foodant comfeirt and prade fon the por indis tetu in gra sites whe the strole for lie was harder then it was here withtust\n",
      "he gave think s for and foodant comfort and place for the put indian ten in go side were the store for lie was harder then it was here withtust\n",
      "he gave think s for a foodant comfort and pride for the poor indies ten in gray sides we the stole for lie was harder then it was here withtust\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "just smell the wild roses they are always so spicy after a rain\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription: (Original -- Spell Correction -- Spell Correction, POS tagging)\n",
      "\n",
      "jus mel a wil roses theya al by es o spi sy ateu ary\n",
      "just my a was roses the and by eyes o she she at a\n",
      "just me a wild roses they a by eyes o she so ate any\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_predictions(index = 0, partition = 'train')\n",
    "get_predictions(index = 1000, partition = 'train')\n",
    "get_predictions(index = 999, partition = 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "Wow! Localities of the sentences seem to \"make sense\" a little bit (e.g. got \"a wild roses\" instead of \"a was roses\" & \"to say the last\" instead of \"to she the last\"). \n",
    "\n",
    "## Future Recommendations\n",
    "One of the major remaining problems is that the predictions seem to include some non-sensical words that combine several words in the true sentence (e.g. \"withtust\" for \"with us\") OR break up a single word in the true sentence (e.g. \"ere markcabl\" for \"remarkable\"). Thus, it would be a good idea to work on inventing an algorithm to improve this in the future. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
